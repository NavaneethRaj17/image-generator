# Image Generator Model Using Generative Adversarial Network(GAN)
A generative AI model involves a dual-network setup consisting of a generator and a discriminator.The generator creates new data samples from random noise, while the discriminator evaluates these samples against real data to classify them as real or fake. The two networks are trained in tandem through an adversarial process where the generator aims to improve its ability to produce realistic outputs, and the discriminator enhances its accuracy in distinguishing between genuine and generated data.

# Generative Adversarial Networks (GANs)
Generative Adversarial Networks (GANs) are a framework for training a system to generate new data, such as images, music, or text, that is indistinguishable from real data.
GANs consist of two main components.The generator and discriminator are trained in an adversarial manner:

<img width="675" alt="GAN" src="https://github.com/user-attachments/assets/6f12b782-a12e-4e13-b3f4-4bce6b7e2881">

* Generator: This network generates new data samples based on random noise as input. It aims to create data that is as realistic as possible.The generator tries to produce data that can fool the discriminator.
* Discriminator: This network is tasked with distinguishing between real data and generated data. It acts as a critic, evaluating the quality of the generator's output.The discriminator learns to accurately identify real and fake data.
Through this competitive process, both networks improve over time. The generator learns to create more realistic data, while the discriminator becomes better at detecting fake data.

# The Basic Setup
We will be using the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist) for building a generative AI modelGenerative Adversarial Networks (GANs) have proven to be remarkably effective in generating high-quality images that are often indistinguishable from real ones.

* Generator:
  * Generates a batch of fake images.
  * The discriminator evaluates the fake images and provides a score.
  * The generator's goal is to maximize the discriminator's score for fake images.
  * Initializing a sequential model. This means the layers will be added one after the other in a linear stack.
  * Applying the Leaky ReLU(Rectified Linear Unit) activation function: A non-linear activation function commonly used in neural networks. It's a variation of the         traditional ReLU function, which sets all negative values to zero.Leaky ReLU allows a small, positive slope for negative inputs. This prevents the "dying ReLU" problem, where neurons can become inactive and stop learning.

* Discriminator:
  * A binary classifier that evaluates a batch of real images and a batch of fake images.
  * The discriminator's goal is to minimize the score for fake images and maximize the score for real images.
  * The model is compiled with the Adam optimizer.The Adam optimizer (short for Adaptive Moment Estimation) is one of the most popular optimization algorithms used in training neural networks:
      * Adam adjusts the learning rate for each parameter individually. This means that parameters with larger gradients get a smaller learning rate and those with smaller gradients get a larger learning rate, allowing for more efficient training.
      * The algorithm includes bias-correction terms to address the issue of initialization. Early in training, the moving averages of the gradients are biased towards zero. Adam corrects this bias to provide more accurate estimates of the gradients.
  * Binary cross-entropy (BCE) is a loss function commonly used in binary classification tasks, including tasks in neural networks like those involving GANs (Generative Adversarial Networks). It quantifies the difference between the predicted probabilities and the actual class labels.

# Defining and training a Generative Adversarial Network by combining a generator and a discriminator into a single model
The discriminator’s weights are set to non-trainable during this process to ensure that only the generator learns from the adversarial feedback.The GAN is trained using the function train_gan, where the discriminator first learns to distinguish between real images and fake images generated by the generator, and then the generator is updated to produce more convincing fake images.

# Image Generation
The model generated several images. Look at these three images in the output below:

<img width="483" alt="epoch zero" src="https://github.com/user-attachments/assets/62049f35-a544-4b8c-b9f5-9575f769c3d8">

The first image represents images of the early outputs. Initially, the images appear as random noise without any discernible patterns. It represents that the generator wasn’t able to learn how to produce meaningful outputs during this training phase.



<img width="517" alt="epoch 1-9000" src="https://github.com/user-attachments/assets/98278be2-8a77-47e7-b5eb-ad24d309ddb2">

The second image represents images from the intermediate output. It shows that as training progresses, the generator starts producing outputs that begin to resemble the structure of handwritten digits.During these stages, the generator and discriminator are in a competitive phase where both networks are improving. The generator tries to create more realistic images, while the discriminator enhances its ability to distinguish between real and generated samples.



<img width="519" alt="epoch 9000-10000" src="https://github.com/user-attachments/assets/4d368b97-99cd-4c65-8af5-3468b797cfb1">

The third image represents images from the later stages of the output generated by the model. The images show a significant improvement, with many outputs clearly resembling real MNIST digits. The details of the digits are more defined, and the shapes are more accurate, which reflects the generator’s increased capability to capture the distribution of the training data.


